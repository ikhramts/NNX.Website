<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
		<head>
		<meta content="en-au" http-equiv="Content-Language" />
		<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
		<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
		<title>Multilayer perceptron</title>
		<link href='https://fonts.googleapis.com/css?family=Montserrat+Alternates:700|Varela+Round' rel='stylesheet' type='text/css'>
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
		<link rel="shortcut icon" type="image/png" href="/favicon-16.png"/>
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">
		<link rel="stylesheet" href="//cdn.jsdelivr.net/highlight.js/8.4/styles/default.min.css">
		<link rel="stylesheet" type="text/css" href="/css/style.css" />
	</head>

	<body>
		<div class="outer-wrap">
			<div id="content">
				<div class="navbar-wrap">
					<nav class="navbar ">
					  <div class="container-fluid">
					    <!-- Brand and toggle get grouped for better mobile display -->
					    <div class="navbar-header">
					      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
					        <span class="sr-only">Toggle navigation</span>
							<i class="fa fa-bars"></i>

					      </button>
					      <div class="navbar-brand" href="index.html">
					      	<p class="h1"><a href="/"><span class="nnx-nn">NN</span><span class="nnx-x">X</span></a></p>
					      	<p class="tagline"><a href="/">Neural networks for Excel</a></p>
					      </div>
					    </div>

					    <!-- Collect the nav links, forms, and other content for toggling -->
					    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
					      <ul class="nav navbar-nav">
					        <li><a href="/index.html"><i class="fa fa-home"></i>  Home</a></li>
							<li>
								<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"><i class="fa fa-book"></i> Docs <span class="caret"></span></a>
					          <ul class="dropdown-menu">
					            <li><a href="/docs/tutorial/">Tutorial</a></li>
					            <li><a href="/docs/reference/">Function reference</a></li>
					            <li><a href="https://github.com/ikhramts/NNX/blob/master/Samples/Iris-flower-data-set.xlsx?raw=true">
									Sample workbook
								</a></li>
					          </ul>
							</li>
							<li>
								<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"><i class="fa fa-comments"></i> Discuss <span class="caret"></span></a>
					          <ul class="dropdown-menu">
					            <li><a href="https://github.com/ikhramts/NNX/issues">Issue tracker</a></li>
								<li><a href="https://groups.google.com/forum/#!forum/nnx-addin">Mailing list</a></li>
					            <li><a href="http://datascience.stackexchange.com/questions/tagged/nnx">StackExchange</a></li>
					          </ul>
							</li>
					        <li><a href="https://github.com/ikhramts/nnx"><i class="fa fa-github"></i>  Source</a></li>
					      </ul>
					    </div><!-- /.navbar-collapse -->
					  </div><!-- /.container-fluid -->
					</nav>
				</div>
				<div class="content-wrap">
					<div class="container">
						<div class="row">
<div class="reference-nav col-md-3">
    <h3>Trainers</h3>
    <ul>
        <li><a href="nnMakeSimpleGradientTrainer.html">nnMakeSimpleGradientTrainer</a></li>
        <li><a href="nnMakeUntilDoneGradientTrainer.html">nnMakeUntilDoneGradientTrainer</a></li>
    </ul>

    <h3>Neural nets</h3>
    <ul>
        <li><a href="MultilayerPerceptron.html">Multilayer perceptron</a>
            <ul class="sublist">
                <li><a href="nnMakeMultilayerPerceptron.html">nnMakeMultilayerPerceptron</a></li>
                <li><a href="nnTrainMultilayerPerceptron.html">nnTrainMultilayerPerceptron</a></li>
            </ul>
        </li>
    </ul>
    
    <h3>Neural net functions</h3>
    <ul>
        <li><a href="nnFeedForward.html">nnFeedForward</a></li>
        <li><a href="nnGetWeights.html">nnGetWeights</a></li>
        <li><a href="nnMakeArray.html">nnMakeArray</a></li>
        <li><a href="nnMakeWeights.html">nnMakeWeights</a></li>
    </ul>

    <h3>Utilities</h3>
    <ul>
        <li><a href="nnClearAllObjects.html">nnClearAllObjects</a></li>
        <li><a href="nnGetCrossEntropyError.html">nnGetCrossEntropyError</a></li>
        <li><a href="nnGetMeanSquareError.html">nnGetMeanSquareError</a></li>
    </ul>
</div>
<div class="reference-body col-md-9">
    <h1>Multilayer perceptron</h1>
    <p>Multilayer perceptron is a basic feed-forward neural network that maps sets of data into output categories. It is best used for classification of inputs into groups.</p>
<p>NNX implementation of multilayer perceptron has the following properties:</p>
<ul>
<li>Every node in a given layer is connected to every node in preceding and succeeding layers.</li>
<li>Hidden layer activation function is \(\tanh\).</li>
<li>Output layer activation function is <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>.</li>
<li>Backpropagation assumes that the error is calculated using <a href="nnGetCrossEntropyError.html">cross-entropy error function</a>.</li>
</ul>
<p>In NNX nomenclature, for multilayer perceptrons (and any other neural networks) the input layer is layer 0, first hidden layer is layer 1, and the first layer of weights is the set of weights for connections between input and hidden layers.  All remaining layers are numberered in increasing order.</p>
<h2>Functions</h2>
<p>Multilayer perceptrons can be created using:</p>
<ul>
<li><a href="nnMakeMultilayerPerceptron.html"><code>nnMakeMultilayerPerceptron</code></a>: directly creates a multilayer perceptron and assigns weights to all connections.</li>
<li><a href="nnTrainMultilayerPerceptron.html"><code>nnTrainMultilayerPerceptron</code></a>: trains a multilayer perceptron given a trainer, a training set, and some additional arguments.</li>
</ul>
<h2>Calculation details</h2>
<h3>Feed forward</h3>
<p>Given:</p>
<ul>
<li>A perceptron with layers \(0, 1, ..., N\), where layer \(0\) is the input layer and layer \(N\) is the output layer,</li>
<li>\(k_a\) nodes per layer for each layer \(0 \leq a \leq N\), with the final \(k_a\)-th node always set to 1 and representing the bias (except for output layer, which does not have bias),</li>
<li>for each layer \(1 \leq a \leq N\), the weights \(\mathbf{w}_a = [w_a (1, 1), w_a (1, 2), ... , w_a (k_{a-1}, k_a - 1)]\) for connections between each node between layer \(a\) and layer \(a - 1\),</li>
<li>input and hidden layer nodes \(h_{i_a}^a\) for \(1 \leq a \leq N-1\) and \(0 \leq i_a \leq k_a\),</li>
</ul>
<p>The feed-forward algorithm for a multilayer perceptron works as follows.  For hidden layers \(1 \leq a \leq N-1\), value of a hidden node \(h_{i_a}^l\) is given by \(\tanh\) of linear combination of node values for the preceding layer:</p>
<p>\begin{equation*}
h_{i_a}^a = \tanh\left( \sum_{j = 1} ^ {k_{a-1}} w_a (j, i_a) h_{j}^{a - 1} \right)
\end{equation*}</p>
<p>This calculation is performed from first hidden layer to last, iteratively using nodes from each calculated layer to determine values for the next layer.</p>
<p>Then, to calculate values for output layer nodes, the multilayer perceptron first calculates a pre-output value \(o_i^\prime\) for each \(1 &lt; i &lt; k_N\) by taking a linear combination of last hidden layer's nodes</p>
<p>\begin{equation*}
o_i^\prime = \sum_{j = 1} ^ {k_{l-1}} w_N (j, i) h_{j}^{N - 1}
\end{equation*}</p>
<p>and uses softmax to calculate the final output values:</p>
<p>\begin{equation*}
o_i = \frac{o_i^\prime}{\sum_{j = 1} ^ {k_N} o_j^\prime }
\end{equation*}</p>
<p>These are the final values that are produced when using <a href="nnFeedForward.html"><code>nnFeedForward()</code></a> on multilayer perceptron.</p>
<h3>Gradient calculation</h3>
<p>The neural network's gradient calculation is used by trainers like <a href="nnMakeSimpleGradientTrainer.html"><code>SimpleGradientTrainer</code></a> to determine the adjustment to make in each backpropagation step.  For this multulayer perceptron, gradient calculation assumes that cross-entropy error is used as the error function that determines how close the neural network got to the expected outputs.</p>
<p>The gradient calculation is given as follows.  Given:</p>
<ul>
<li>\(E\): error function (cross-entropy error of targets relative to outputs).</li>
<li>\(N\): number of hidden and output layers.</li>
<li>\(a\): a layer between \(0\) (input) and \(n\) (output), inclusive.</li>
<li>\(i_a\): \(i\)-th node in layer \(a\).</li>
<li>\( \mathbf{o} = (o_1, ..., o_{i_N})\): neural network outputs for the given inputs.</li>
<li>\( \mathbf{t} = (t_1, ..., t_{i_N})\): neural network training targets for the given inputs.</li>
<li>\(h_{i_a}\): value of \(i\)-th hidden (or input, or output) node in layer \(a\) after activation.</li>
<li>\(h_{i_a}^\prime\): value of \(i\)-th node in layer \(a\) before activation (so \(h_{i_a} = \tanh h_{i_a}^\prime\)) .</li>
<li>\(w_a (i_{a-1}, i_a)\): weight of \(i_{a-1}\)-th node in layer \(a-1\) in activation of \(i_a\)-th node in layer \(a\).</li>
<li>\(\mathbf{w} = (w_1(1, 1), ..., w_N (k_{N-1}, k_N))\): collection of all weights in the neural network.</li>
</ul>
<p>Note that indexes \(i_a\) and \(i_{a-1}\) may have different values.</p>
<p>Then, for each weight \(w_a (i_{a-1}, i_a)\), the partial derivative of the error function with respect to the weight is given by</p>
<p>\begin{equation*}
\frac{\partial E}{\partial w_a (i_{a-1},i_a)} = h_{i_{a-1}}\frac{\partial E}{\partial h_{i_{a}^\prime}}
\end{equation*}</p>
<p>where for layer \(N\) (output)</p>
<p>\begin{equation*}
\frac{\partial E}{\partial h^\prime_{i_{n}}} = o_{i_n} - t_{i_n}
\end{equation*}</p>
<p>and for any preceding layers \(1 \leq a &lt; N\) it is given recursively by</p>
<p>\begin{equation*}
\frac{\partial E}{\partial h_{i_{a}^\prime}} = (1-h^2_{i_a})\sum_{j_{a+1}}w_a (i_a, j_{a+1})\frac{\partial E}{\partial h_{j_{a+1}}^\prime}
\end{equation*}</p>
<p>The overall gradient is then the collection of partial derivatives of \(E\) with respect to each weight:</p>
<p>\begin{equation*}
\nabla E(\mathbf{w}) = \left( \frac{\partial E}{\partial w_1(1,1)}, ...,  \frac{\partial E}{\partial w_N(k_{N-1}, k_N)} \right)
\end{equation*}</p>
<h2>Layout of weights</h2>
<p>This section may be useful if you wish to directly provide the weights for multilayer perceptron, or if you wish to figure out what the result of <a href="nnGetWeights.html"><code>nnGetWeights()</code></a> produces.</p>
<p>The weights of the perceptron are represented as an array of arrays, with each internal array representing an individual layer of connections between the layers.  The first array represents the weights of the the connections between the input layer and the first hidden layer, and subsequent arrays represent connections between later layers, with final array of weights being the weights of connections between final hidden layer and output layer.</p>
<p>Except for the final weights array that leads to outputs, each weights array has the following layout:</p>
<p>$$
[w(1, 1), w(2, 1), w(3, 1), ... w(k_{a-1}, 1), w(2, 1), w(2, 2), ..., w(k_{a-1}, k_a - 1)]
$$</p>
<p>Here \(w(i, j)\) is the weight of the connection between \(i\)-th node in preceding layer (layer \(a-1\)) and \(j\)-th node in the next layer (layer \(a\)).  Note that if layer \(a\) is a hidden layer, then the second index \(j\) only goes up to \(k_a - 1\) because \(k_a\)-th node is the bias node which is always set to 1.  This also means that weights \(w(k_{a-1}, j)\) for all \(j\) are bias weights leading into each node \(j\) in layer \(a\).</p>
<p>In case where layer the next layer \(a\) is the output layer, the second index will go up to \(k_a\) instead of \(k_a - 1\) because the output layer does not have bias nodes:</p>
<p>$$
[w(1, 1), w(2, 1), w(3, 1), ... w(k_{a-1}, 1), w(2, 1), w(2, 2), ..., w(k_{a-1}, k_a)]
$$</p>
</div>
</div>
					</div>
				</div>
			</div>
		</div>

		<div class="footer-wrap">
			<div class="footer">
				<p>Copyright 2016 Iouri Khramtsov.</p>
				<p>
					Code and binaries are licensed under <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a>,
					documentation licenced under <a href="http://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>.
				</p>
			</div>
		</div>

		<script src="//cdn.jsdelivr.net/highlight.js/8.4/highlight.min.js"></script>
		<script>hljs.initHighlightingOnLoad();</script>
		<script src="https://code.jquery.com/jquery-2.2.0.min.js" crossorigin="anonymous"></script>
		<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>
		<script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({
			  TeX: { equationNumbers: { autoNumber: "AMS" } },
			  "HTML-CSS": {
			    preferredFont: "Latin-Modern",
				webFont: "Latin-Modern"
			},
			  SVG: {
				  font: "Latin-Modern"
			  }
			});
		</script>
		<script>
		  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		  ga('create', 'UA-16974319-6', 'auto');
		  ga('send', 'pageview');
		</script>
	</body>
</html>
