<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
		<head>
		<meta content="en-au" http-equiv="Content-Language" />
		<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
		<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
		<title>nnMakeSimpleGradientTrainer</title>
		<link href='https://fonts.googleapis.com/css?family=Montserrat+Alternates:700|Varela+Round' rel='stylesheet' type='text/css'>
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
		<link rel="shortcut icon" type="image/png" href="/favicon-16.png"/>
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">
		<link rel="stylesheet" href="//cdn.jsdelivr.net/highlight.js/8.4/styles/default.min.css">
		<link rel="stylesheet" type="text/css" href="/css/style.css" />
	</head>

	<body>
		<div class="outer-wrap">
			<div id="content">
				<div class="navbar-wrap">
					<nav class="navbar ">
					  <div class="container-fluid">
					    <!-- Brand and toggle get grouped for better mobile display -->
					    <div class="navbar-header">
					      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
					        <span class="sr-only">Toggle navigation</span>
							<i class="fa fa-bars"></i>

					      </button>
					      <div class="navbar-brand" href="index.html">
					      	<p class="h1"><a href="/"><span class="nnx-nn">NN</span><span class="nnx-x">X</span></a></p>
					      	<p class="tagline"><a href="/">Neural networks for Excel</a></p>
					      </div>
					    </div>

					    <!-- Collect the nav links, forms, and other content for toggling -->
					    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
					      <ul class="nav navbar-nav">
					        <li><a href="/"><i class="fa fa-home"></i>  Home</a></li>
							<li>
								<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"><i class="fa fa-book"></i> Docs <span class="caret"></span></a>
					          <ul class="dropdown-menu">
					            <li><a href="/docs/tutorial/">Tutorial</a></li>
					            <li><a href="/docs/reference/">Function reference</a></li>
					            <li><a href="https://github.com/ikhramts/NNX/blob/master/Samples/Iris-flower-data-set.xlsx?raw=true">
									Sample workbook
								</a></li>
					          </ul>
							</li>
							<li>
								<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"><i class="fa fa-comments"></i> Discuss <span class="caret"></span></a>
					          <ul class="dropdown-menu">
					            <li><a href="https://github.com/ikhramts/NNX/issues">Issue tracker</a></li>
								<li><a href="https://groups.google.com/forum/#!forum/nnx-addin">Mailing list</a></li>
					            <li><a href="http://datascience.stackexchange.com/questions/tagged/nnx">StackExchange</a></li>
					          </ul>
							</li>
					        <li><a href="https://github.com/ikhramts/nnx"><i class="fa fa-github"></i>  Source</a></li>
					      </ul>
					    </div><!-- /.navbar-collapse -->
					  </div><!-- /.container-fluid -->
					</nav>
				</div>
				<div class="content-wrap">
					<div class="container">
						<div class="row">
<div class="reference-nav col-md-3">
    <h3>Trainers</h3>
    <ul>
        <li><a href="nnMakeSimpleGradientTrainer.html">nnMakeSimpleGradientTrainer</a></li>
        <li><a href="nnMakeUntilDoneGradientTrainer.html">nnMakeUntilDoneGradientTrainer</a></li>
    </ul>

    <h3>Neural nets</h3>
    <ul>
        <li><a href="MultilayerPerceptron.html">Multilayer perceptron</a>
            <ul class="sublist">
                <li><a href="nnMakeMultilayerPerceptron.html">nnMakeMultilayerPerceptron</a></li>
                <li><a href="nnTrainMultilayerPerceptron.html">nnTrainMultilayerPerceptron</a></li>
            </ul>
        </li>
    </ul>
    
    <h3>Neural net functions</h3>
    <ul>
        <li><a href="nnFeedForward.html">nnFeedForward</a></li>
        <li><a href="nnGetWeights.html">nnGetWeights</a></li>
        <li><a href="nnMakeArray.html">nnMakeArray</a></li>
        <li><a href="nnMakeWeights.html">nnMakeWeights</a></li>
    </ul>

    <h3>Utilities</h3>
    <ul>
        <li><a href="nnClearAllObjects.html">nnClearAllObjects</a></li>
        <li><a href="nnGetCrossEntropyError.html">nnGetCrossEntropyError</a></li>
        <li><a href="nnGetMeanSquareError.html">nnGetMeanSquareError</a></li>
    </ul>
</div>
<div class="reference-body col-md-9">
    <h1>nnMakeSimpleGradientTrainer</h1>
    <p><code>SimpleGradientTrainer</code> trains neural networks by applying <code>NumEpochs</code> backpropagation steps, calculating the gradient of the error function with respect to weights at every step and moving the weights in that direction, with some adjustments.  Scroll below for details.</p>
<h2>Inputs/outputs</h2>
<table>
<thead>
<tr>
<th>Arguments</th>
<th style="text-align:center"></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Name</code></td>
<td style="text-align:center">string, not empty</td>
<td>Name of the trainer object to create.</td>
</tr>
<tr>
<td><code>NumEpochs</code></td>
<td style="text-align:center">integer, &gt;0</td>
<td>Number of backpropagation steps to perform during training. Each backpropagation step includes evaluating a batch of one or more training points.</td>
</tr>
<tr>
<td><code>LearningRate</code></td>
<td style="text-align:center">number, &gt;0</td>
<td>Used to determine the impact of gradient calculated in each backpropagation step on the neural network's weights.</td>
</tr>
<tr>
<td><code>Momentum</code></td>
<td style="text-align:center">number, ≥0</td>
<td>Used to determine the impact of gradient calculated in <em>previous</em> backpropagation step on the neural network's weights.</td>
</tr>
<tr>
<td><code>QuadraticRegularization</code></td>
<td style="text-align:center">number, ≥0</td>
<td>Higher values force weights to stay closer to 0, hopefully preventing overfitting.</td>
</tr>
<tr>
<td><code>BatchSize</code></td>
<td style="text-align:center">integer, &gt;0</td>
<td>Number of training samples to include in every epoch.</td>
</tr>
<tr>
<td><code>Seed</code></td>
<td style="text-align:center">integer, ≥0</td>
<td>Seed for random number generator.  Training runs with the same seed and same input parameters should always produce identical results.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Returns</th>
</tr>
</thead>
<tbody>
<tr>
<td>Name of the newly created trainer object.</td>
</tr>
</tbody>
</table>
<h2>Details</h2>
<p>For every backpropagation step \(1 \leq t \leq \) <code>NumEpochs</code>, <code>SimpleGradientTrainer</code> selects <code>BatchSize</code> samples from the training set (at random, with replacement), and calculates the average of gradients of the network's error for each sample with respect to weights to arrive at the estimated overall gradient of network's error given its current weights, \(\nabla E(\mathbf{w}_{t-1})\).  The trainer then adds adjustments from <code>Momentum</code> (\(m\)) and <code>QuadraticRegularization</code> (\(q\)) to arrive at adjusted gradient \(\nabla_t^\prime\) to apply to the weights in this step:</p>
<p>\begin{equation}
\nabla_t^\prime = \nabla E(\mathbf{w}_{t-1}) + m \nabla E(\mathbf{w}_{t-2}) + q \mathbf{w}_{t-1}
\end{equation}</p>
<p>The trainer then uses the adjusted gradient to calculate the new estimate for weights:</p>
<p>\begin{equation}
\mathbf{w}_t = \mathbf{w}_{t-1} - \mathrm{LearningRate} \cdot \nabla_t^\prime
\end{equation}</p>
<p>These weights are then assigned to the neural network for use in the next backpropagation step.</p>
<h3>Meaning of learning rate, momentum, quadratic regularization</h3>
<p>In formula (1) above, <code>Momentum</code> (\(m\)) term may help lessen effect of oscillations across a valley by adding previous step's gradient to this step's gradient,  though high momentums will lead to slower convergence.</p>
<p>The <code>QuadraticRegularization</code> (\(q\)) term forces the gradient to point increasingly toward zero as the weights become bigger, thus preventing the weights from becoming too large or too small.  This lowers risk of overfitting, and ensures that the weights remain in the range where they are sensitive enough to corrections in future steps.</p>
<p>In formula (2), <code>LearningRate</code> determines the overall impact of the adjustment in each step, with higher values leading to higher adjustments, but also possibly leading to overshooting the optimum weights.</p>
<h3>Role of the neural network</h3>
<p>The formulas above have purposefully omitted details on how to calculate the neural network's error gradient, \(\nabla E(\mathbf{w}_{t-1})\).  Like any other trainer in NNX, <code>SimpleGradientTrainer</code> is a generic trainer that can act on any neural network supported by NNX; the specifics of calculating the gradient are therefore provided by the neural network that the trainer acts on. These details can be found on reference page for the neural network, e.g. <a href="MultilayerPerceptron.html">Multilayer perceptron</a>.</p>
</div>
</div>
					</div>
				</div>
			</div>
		</div>

		<div class="footer-wrap">
			<div class="footer">
				<p>Copyright 2016 Iouri Khramtsov.</p>
				<p>
					Code and binaries are licensed under <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a>,
					documentation licenced under <a href="http://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>.
				</p>
			</div>
		</div>

		<script src="//cdn.jsdelivr.net/highlight.js/8.4/highlight.min.js"></script>
		<script>hljs.initHighlightingOnLoad();</script>
		<script src="https://code.jquery.com/jquery-2.2.0.min.js" crossorigin="anonymous"></script>
		<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>
		<script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({
			  TeX: { equationNumbers: { autoNumber: "AMS" } },
			  "HTML-CSS": {
			    preferredFont: "Latin-Modern",
				webFont: "Latin-Modern"
			},
			  SVG: {
				  font: "Latin-Modern"
			  }
			});
		</script>
		<script>
		  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		  ga('create', 'UA-16974319-6', 'auto');
		  ga('send', 'pageview');
		</script>
	</body>
</html>
